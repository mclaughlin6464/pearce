#!/bin/bash
"""The Emu object esentially wraps the George gaussian process code. It handles building, training, and predicting."""

import warnings
from glob import glob
from itertools import izip
from multiprocessing import cpu_count
from os import path
from time import time
from abc import ABCMeta, abstractmethod

import emcee as mc
import numpy as np
import george
from george.kernels import *
import scipy.optimize as op
from scipy.interpolate import interp1d
from scipy.linalg import inv, block_diag
from scipy.spatial import KDTree
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.kernel_ridge import KernelRidge
from sklearn.svm import SVR

from .ioHelpers import *


class Emu(object):
    '''Main Emulator base class. Cannot itself be instatiated; can only be accessed via subclasses.
       controls all loading, manipulation, and emulation of data.'''

    __metaclass__ = ABCMeta
    valid_methods = {'gp', 'svr', 'gbdt', 'rf', 'krr'}  # could add more, coud even check if they exist in sklearn

    def __init__(self, training_dir, method='gp', hyperparams={}, fixed_params={}, independent_variable=None):
        '''
        Initialize the Emu
        :param training_dir:
            Directory containing training data, in the format generated by trainingData.py
        :param method:
            Emulation method. Valid methods are 'gp', 'svr', 'gbdt', 'rf', and 'krr'. Default is 'gp'. GP is
            conducted by george, all others are executed by sklearn. Kernel based methods use a george kernel.
        :param hyperparams:
            Hyperparameters for the emulator. Gp hyperparams are passed into george, others are passed into
            sklearn. A special hyperparam is 'metric', which determines the metric for kernel-based methods.
             See documentation for a full list of hyperparameters.
             Default is {}.
        :param fixed_params:
            Parameters to hold fixed during training. Key is the name of the param, and value is the fixed value.
             Default is {}.
        :param independent_variable:
            Indepent variable to emulate. Default is None, which just emulates the iv in the training data
            directly. Presently the only acceptable option is 'r2', which emulates r^2 times the
            parameter in the training data.
        '''

        assert method in self.valid_methods

        if independent_variable == 'bias':
            raise NotImplementedError("I have to work on how to do xi_mm first.")
        assert independent_variable in {None, 'r2'}  # no bias for now.

        # TODO I hate the assembly bias parameter keys. It'd be nice if the use could pass in something
        # else and I make a change

        self.method = method

        self.fixed_params = fixed_params
        self.independent_variable = independent_variable

        self.load_training_data(training_dir)
        self.build_emulator(hyperparams)

    ###Data Loading and Manipulation####################################################################################
    def get_data(self, data_dir, em_params, fixed_params, independent_variable):
        """
        Read data in the format compatible with this object and return it
        :param data_dir:
            Directory where data from trainingData is stored
        :param em_params:
            Parameters held fixed by the emulator. Slightly different than fixed_params, used for plotting.
            NOTE Sort of different than em_params accepted by the emulator. If comparing emulator predictions to
            truth on a plot, passing in em_params used for the emulator to this function will return data values
            at em_params.
        :param fixed_params:
            Parameters to hold fixed. Only available if data in data_dir is a full hypercube, not a latin hypercube
        :param independent_variable:
            Independant variable to emulate. Options are xi, r2xi, and bias (eventually)..
        :return: x, y, yerr, ycov, all numpy arrays.
                 x is (n_data_points, n_params)
                 y is (n_data_points, ), yerr is (n_data_points)
                 and ycov (n_scale_bins, n_scale_bins), the average covariance matrix
        """

        input_params = {}
        input_params.update(em_params)
        input_params.update(fixed_params)

        # If redshfit is fixed, we don't need to open the subdirs for other redshifts.
        sub_dirs, sub_dirs_as = self._get_z_subdirs(data_dir, fixed_zs=fixed_params.get('z', None))
        # we store redshifts, not scale factors
        self.redshift_bin_centers = 1 / sub_dirs_as - 1

        all_x, all_y, all_yerr, all_ycov = [], [], [], []

        for sub_dir, z in izip(sub_dirs, self.redshift_bin_centers):

            bins, cosmo_params, obs, sampling_method = global_file_reader(sub_dir)
            self.obs = obs

            # Depending on if this is a full or latin hypercube, and if params are supposed to be fixed, do different
            # things.
            # Could add an assert that a = cosmo_params['scale_factor']
            if fixed_params and fixed_params.keys() != ['z']:
                fp_keys = set(fixed_params.keys())
                # could maybe do this with sets
                # Problem is r and z are both acceptable in an LHC, no others.
                valid_LHC_fp = fp_keys.issubset(set(['r','z']))
                if sampling_method == 'LHC' and  not valid_LHC_fp:  # not allowed
                    raise ValueError('Fixed parameters is not empty, but the data in data_dir is form a Latin Hypercube. \
                                    Cannot performs slices on a LHC.')
                else:  # FHC, load up the training file locations
                    ordered_params_with_fixed = self._get_ordered_params(sub_dir, fixed_params, with_fixed_params=True)
                    training_file_loc = training_file_loc_reader(sub_dir)
                    obs_files, cov_files = self._get_fixed_files(training_file_loc, ordered_params_with_fixed,
                                                                 input_params, sub_dir)
            else:  # look at all of them.
                # sorted to ensure parameters line up correctly
                self._get_ordered_params(sub_dir, fixed_params)
                obs_files = sorted(glob(path.join(sub_dir, 'obs*.npy')))
                cov_files = sorted(glob(path.join(sub_dir, 'cov*.npy')))

            for key in em_params:  # assert defined params are in ordered
                assert any([pname == key for pname in self._ordered_params])

            # store the binning for the scale_bins
            # assumes that it's the same for each box, which should be true.
            if 'r' not in fixed_params:
                scale_bin_centers = (bins[:-1] + bins[1:]) / 2
                scale_nbins = scale_bin_centers.shape[0]
                npoints = len(obs_files) * scale_nbins  # each file contains NBINS points in r, and each file is a 6-d point
            else:
                tmp_scale_bin_centers = (bins[:-1] + bins[1:]) / 2
                try:
                    assert fixed_params['r'] in tmp_scale_bin_centers
                except AssertionError:
                    raise AssertionError("r=%.2f is not a valid fixed parameter value."%fixed_params['r'])
                scale_bin_centers = np.array([fixed_params['r']])
                scale_nbins = 1
                npoints = len(obs_files)
                rbin_idx = np.argwhere(fixed_params['r'] == tmp_scale_bin_centers)

            if hasattr(self, "scale_bin_centers"):
                if scale_bin_centers.shape != self.scale_bin_centers.shape or \
                        np.any(scale_bin_centers != self.scale_bin_centers):
                    warnings.warn(
                        "The scale bin centers in %s are not the same as the ones alread attached to this object. This may lead to weird behavior!" % sub_dir)

            ndim = len(self._ordered_params)
            x = np.zeros((npoints, ndim))
            y = np.zeros((npoints,))
            yerr = np.zeros((npoints,))
            ycov = np.zeros((scale_nbins, scale_nbins))

            warned = False
            num_skipped = 0
            num_used = 0

            for idx, (obs_file, cov_file) in enumerate(izip(obs_files, cov_files)):
                params, obs, cov = obs_file_reader(obs_file, cov_file)

                if 'r' in fixed_params:
                    obs = obs[rbin_idx]
                    cov = cov[rbin_idx,rbin_idx]

                # skip NaNs
                if np.any(np.isnan(cov)) or np.any(np.isnan(obs)):
                    # only warn once.
                    if not warned:
                        warnings.warn('WARNING: NaN detected. Skipping point in %s' % cov_file)
                        warned = True
                    num_skipped += 1
                    continue

                num_used += 1

                # take the params from the file and put them in the right order and right place
                file_params = []
                for pname in self._ordered_params:
                    if pname not in fixed_params and pname not in {'z', 'r'}:  # may need to be input_params
                        # note we have to repeat for each scale bin
                        file_params.append(np.ones((scale_nbins,)) * params[pname])

                # r and z will always be at the end.
                if 'z' not in fixed_params:
                    file_params.append(np.ones((scale_nbins,)) * z)
                if 'r' not in fixed_params:
                    file_params.append(np.log10(scale_bin_centers))

                x[idx * scale_nbins:(idx + 1) * scale_nbins, :] = np.stack(file_params).T

                y[idx * scale_nbins:(idx + 1) * scale_nbins], _cov = self._iv_transform(independent_variable, obs, cov)
                yerr[idx * scale_nbins:(idx + 1) * scale_nbins] = np.sqrt(np.diag(_cov))
                ycov += _cov

            # remove rows that were skipped due to the fixed thing
            # NOTE: HACK
            # a reshape may be faster.
            # TODO could I use Nonzero, or, better, keep track of the index I need??
            # Yeah isn't this just num_used?
            # No, cuz there can be gaps. Also it'd be num_used * n_scalebins
            zeros_slice = np.any(x != 0.0, axis=1)

            all_x.append(x[zeros_slice])
            all_y.append(y[zeros_slice])
            all_yerr.append(yerr[zeros_slice])
            all_ycov.append(ycov/num_used) #add the average

        self.scale_bin_centers = scale_bin_centers

        if 'r' in self._ordered_params and self._ordered_params['r'] == (0,1):
            self._ordered_params['r'] = (np.min(self.scale_bin_centers), np.max(self.scale_bin_centers))

        if 'z' in self._ordered_params and self._ordered_params['z'] == (0,1):
            self._ordered_params['z'] = (np.min(self.redshift_bin_centers), np.max(self.redshift_bin_centers))

        # TODO sort?
        return np.vstack(all_x), np.hstack(all_y), np.hstack(all_yerr), np.vstack(all_ycov).mean(axis = 0)

    def get_plot_data(self, em_params, training_dir, independent_variable=None, fixed_params={},
                      dependent_variable='r'):
        """
        Similar function to load_training_data. However, returns values for plotting comparisons to the emulator.
        :param em_params:
            Similar to fixed params. A dictionary of values held fixed in the emulator, as opposed to fixed_params
            which are values held fixed in the training data.
        :param training_dir:
            Directory where training data from trainginData is stored.
        :param independent_variable:
            Independant variable to emulate. Options are xi, r2xi, and bias (eventually).
        :param fixed_params:
            Parameters to hold fixed. Only available if data in training_dir is a full hypercube, not a latin hypercube.
        :param dependent_variable:
            "x axis" variable. Default is 'r', but 'z' is also acceptable.
        :return: log_r (or z), y, yerr for the independent variable at the points specified by fixed nad em params.
        """

        x, y,yerr, _ = self.get_data(training_dir, em_params, fixed_params, independent_variable)

        sort_idxs = self._sort_params(x, argsort=True)

        log_bin_centers = np.log10(self.scale_bin_centers) if dependent_variable == 'r' else self.redshift_bin_centers
        # repeat for each row of y
        log_bin_centers = np.tile(log_bin_centers, sort_idxs.shape[0] / len(log_bin_centers))

        # TODO do I want to reshape to y.shape((-1, log_bin_centers.shape(1)) ?
        return log_bin_centers, y[sort_idxs], yerr[sort_idxs]

    def load_training_data(self, training_dir):
        """
        Read the training data for the emulator and attach it to the object.
        :param training_dir:
            Directory where training data from trainginData is stored. May also be a list of several points.
        :param fixed_params:
            Parameters to hold fixed. Only available if data in training_dir is a full hypercube, not a latin hypercube.
        :return: None
        """
        if type(training_dir) is not list:
            training_dir = [training_dir]

        xs, ys,yerrs, ycovs = [], [], [], []
        for td in training_dir:
            x, y,yerr, ycov = self.get_data(td, {}, self.fixed_params, self.independent_variable)
            xs.append(x)
            ys.append(y)
            ycovs.append(ycov)

        self.x = np.vstack(xs)
        # hstack for 1-D
        self.y = np.hstack(ys)
        self.yerr = yerr
        # TODO delete?
        self.ycov = np.vstack(ycovs).mean(axis = 0)

        # for now, no mean subtraction
        #self.y_hat = np.zeros(self.y.shape[1]) if len(y.shape) > 1 else 0  #
        self.y_hat = self.y.mean(axis = 0)
        self.y -= self.y_hat

        ndim = self.x.shape[1]
        self.fixed_ndim = len(self.fixed_params)
        self.emulator_ndim = ndim  # The number of params for the emulator is different than those in sampling.

    def get_param_names(self):
        """
        Helper function that returns the names of the parameters in the emulator.
        :return: names, a list of parameter names (in order)
        """
        return self._ordered_params.keys()

    def get_param_bounds(self, param):
        """
        Return the emulator bounds for parameter param. Raises a ValueError if it can't be found.
        :param param:
            String, the name of the parameter to return bounds for.
        :return: bounds, a 2 element tuple with the lower and higher bounds of point param.
        """
        try:
            return self._ordered_params[param]
        except KeyError:
            raise KeyError("Parameter %s could not be found." % param)

    def _get_ordered_params(self, dirname, fixed_params, with_fixed_params=False):
        """
        Load the ordered params from directory 'dirname.' Remove params that
        are held fixed by the emulator. Check that there are no violations and,
        if it is not currently attached to the object, attach it. Optionally return
        a version that does not have the fixed parameters removed, which is necessary for some functions
        :param dirname:
            name of the directory to load ordered_params from
        :param fixed_params:
            Iterator of fixed parameters. Will be removed from ordered params
        :param with_fixed_params:
            Boolean. Whether to return the ordered params without the fixed_params removed.
            This is unfrotunately a little hacky, but it is necessary for the training_file_loc
            feature.
        :return: if with_fixed_params, ordered_params_with_fixed_params, a version of the attached object with the fixed
        params left in place.
        """
        # get the defined param ordered for the training data, and ensure its ok.
        ordered_params = params_file_reader(dirname)

        # this will not contain 'z' or 'r'. Add them to the end.
        # note that the bounds of these parameters will be updated later in get_data, if not here.
        if hasattr(self, "redshift_bin_centers") and hasattr(self, "scale_bin_centers"):
            # if we know the actual bounds, attach them
            ordered_params['z'] = (np.min(self.redshift_bin_centers), np.max(self.redshift_bin_centers))
            ordered_params['r'] = (np.min(self.scale_bin_centers), np.max(self.scale_bin_centers))
        else:
            ordered_params['z'] = (0,1)
            ordered_params['r'] = (0,1)

        # store in case we have to return this
        op_with_fixed = ordered_params.copy()

        for pname in fixed_params:
            if pname in ordered_params:
                del ordered_params[pname]
            else:
                raise KeyError('Parameter %s is in fixed_params but not in ordered_params!'%pname)

        attached_or = getattr(self, "_ordered_params", None)
        # attach the ordered params if its new. Otherwise, ensure that the ordering in this dir is the same as what
        # we have already.
        if attached_or is None:
            self._ordered_params = ordered_params
        else:
            try:
                assert ordered_params == attached_or
            except AssertionError:
                raise AssertionError("ordered_params in %s did not match the value attached to this object." % dirname)

        if with_fixed_params:
            return op_with_fixed

    # TODO Should I unify some syntax between this and the one below?
    def check_param_names(self, param_names, ignore=[]):
        """
        Checks that all parameter names in param_names are defined in the emulator, and vice versa.
        :param param_names:
            List of strings of parameter names to compare to _ordered_params
        :param ignore"
            A list of parameters to ignore. There are many use cases where parameters like 'r' are definedi n the
            emulator but not used in the emulator, so we wouldn't want params to define them.
        :return:
            True if all param_names are in ordered_params, and vice verse. False otherwise
        """
        op_set = set(self._ordered_params.iterkeys())
        for ig in ignore:
            op_set.remove(ig)

        ip_set = set(param_names)

        return len(op_set ^ ip_set) == 0

    def _check_params(self, params):
        """
        Assert that all keys in params are defined in ordered params, and vice versa. Raises an AssertionError otherwise.
        Will also raise a warning if trying to emulate out of bounds.
        :param params:
            Dictionary of params, where the key is the name and the value is the value it holds. Value can also
            be a numpy array.
        :return: None
        """
        try:
            assert self.check_param_names(params.keys())
        except AssertionError:
            output = "The input_params passed into get_data did not match those the Emu knows about. \
                                              It's possible fixed_params is missing a parameter, or you defined an extra one. \
                                              Additionally, orded_params could be wrong too!\n"

            op_set = set(self._ordered_params.iterkeys())
            ip_set = set(params.iterkeys())

            ip_not_op = ip_set - op_set
            op_not_ip = op_set - ip_set

            if ip_not_op:
                output += 'Param %s was in the input but not the training data.\n' % list(ip_not_op)[0]
            if op_not_ip:
                output += 'Param %s was in the training data but not the input.\n' % list(op_not_ip)[0]

            raise AssertionError(output)

        for pname, (plow, phigh) in self._ordered_params.iteritems():
            try:
                # check if they're in bounds, else raise an informative warning
                val = params[pname]
                # TODO wish i didn't have to hardcode this
                if pname == 'r':
                    val = 10**val

                assert np.all(plow <= val) and np.all(val <= phigh)
            except AssertionError:
                if type(val) is float:
                    warnings.warn("Emulator value for %s %.3f is outside the bounds (%.3f, %.3f) of the emulator." % (
                    pname, val, plow, phigh))
                else:
                    warnings.warn("One value for %s is outside the bounds (%.3f, %.3f) of the emulator." % (
                    pname, plow, phigh))

    def _get_z_subdirs(self, dirname, fixed_zs=None):
        """
        Quick helper function. Takes a directory with subdirectories labeled by their scale factors
         and the redshifts to hold fixed.
        Finds the corresponding sub_dirs. If it can't find one, raises IOError.
        :param dirname
            Name of the directory to find relevant sub directories
        :param fixed_zs:
            List or float of redshifts to hold fixed.
        :return: sub_dirs, sub_dirs_as. The list of subdirectories, and the corresponding scale factors
        """

        tol = 0.05

        sub_dirs = glob(path.join(dirname, 'a_*'))
        sub_dirs_as = np.array([float(fname[-7:]) for fname in sub_dirs])

        # sort them for consistancy
        sort_idxs = np.argsort(sub_dirs_as)
        sub_dirs_as = sub_dirs_as[sort_idxs]
        sub_dirs = [sub_dirs[i] for i in sort_idxs]

        if fixed_zs is None:
            return sub_dirs, sub_dirs_as

        if type(fixed_zs) is float:
            fixed_zs = [fixed_zs]
        fixed_sub_dirs = []
        fixed_sub_dirs_as = []
        # check the fixed z's against the a's we have in training data
        # only keep dirs that match
        for z in fixed_zs:
            input_a = 1 / (1 + z)
            idx = np.argmin(np.abs(sub_dirs_as - input_a))
            a = sub_dirs_as[idx]
            if np.abs(a - input_a) > tol:  # tolerance
                raise IOError('No subfolder within %f tolerance of z=%.3f' % (tol, z))
            fixed_sub_dirs.append(sub_dirs[idx])
            fixed_sub_dirs_as.append(a)
        return fixed_sub_dirs, np.array(fixed_sub_dirs_as)

    def _get_fixed_files(self, training_file_loc, ordered_params, fixed_params, dirname=''):
        """
        Return the files that satisfy the constraints in fixed_params.
        :param training_file_loc:
            dictionary where the keys are tuples of the parameters of a file, and the value is the corresponding file.
        :param ordered_params:
            An ordered dict with keys as the parameter names and values as their bounds.
            Note must contain fixed parameters too, which are normally removed during initialization.
        :param fixed_params:
            dictionary of parameters to hold fixed. Only files with these values will be returned.
        :param dirname
            optional. The directory the file is located, so this can return an absolute filename. Default is '', which
            means this will return relative filenames.
        :return: obs_files, cov_files. Two lists of filenames where the observed data and the covariance are stored.
        """
        # store the idxs and values so we can look in the tuples without searching each time.
        idx_fixed_val = {}
        for idx, pname in enumerate(ordered_params):
            if pname in fixed_params and pname not in {'z', 'r'}:
                idx_fixed_val[idx] = fixed_params[pname]

        obs_files, cov_files = [], []

        for params, fbase in training_file_loc.iteritems():
            # bit of a spaghetti call
            # If all values are not near the values being held fixed, continue
            if np.any([np.abs(idx_fixed_val[idx] - val) > 1e-6 for idx, val in enumerate(params) if
                       idx in idx_fixed_val]):
                continue

            obs_files.append(path.join(dirname, 'obs_' + fbase))
            cov_files.append(path.join(dirname, 'cov_' + fbase))

        # if no files satisfy this constraint, raise an error showing what values would work.
        if len(obs_files) == 0:
            possible_vals = set()
            for params in training_file_loc:
                possible_vals.add(tuple([params[idx] for idx in idx_fixed_val]))

            raise IOError("No files found with the fixed parameters defined!\nValues available are: " + ' '.join(
                [str(v) for v in possible_vals]))

        return sorted(obs_files), sorted(cov_files)

    def _iv_transform(self, independent_variable, obs, cov):
        """
        Independent variable tranform. Helper function that consolidates this operation all in one place.
        :param independent_variable:
            Which iv to transform to. Current options are None (just take log) and r2.
        :param obs:
            Observable to transform (xi, wprp, etc.)
        :param cov:
            Covariance of obs
        :return:
            y, y_cov the transformed iv's for the emulator
        """
        if independent_variable is None:
            #y = np.log10(obs)
            # Approximately true, may need to revisit
            # yerr[idx * NBINS:(idx + 1) * NBINS] = np.sqrt(np.diag(cov)) / (xi * np.log(10))
            #y_cov = cov/np.outer(obs * np.log(10), obs*np.log(10))
            y = obs
            y_cov = cov

        elif independent_variable == 'r2':  # r2xi
            y = obs * self.scale_bin_centers * self.scale_bin_centers
            y_cov = cov* np.outer(self.scale_bin_centers, self.scale_bin_centers)
        else:
            raise ValueError('Invalid independent variable %s' % independent_variable)

        # if independent_variable == 'bias':
        #    y[idx * NBINS:(idx + 1) * NBINS] = xi / xi_mm
        #    ycovs.append(cov / np.outer(xi_mm, xi_mm))

        return y, y_cov

    def _sort_params(self, t, argsort=False):
        """
        Sort the parameters in a defined away given the orderering.
        :param t:
            Parameter vector to sort. Should have dims (N, N_params) and be in the order
            defined by ordered_params
        :param argsort:
            If true, return indicies that would sort the array rather than the sorted array itself.
            Default is False.
        :return:
            If not argsort, returns the sorted array by column and row. 
            If argsort, return the indicies that would sort the array.
        """
        if t.shape[0] == 1:
            if argsort:
                return np.array([0])
            return t  # a single row array is already sorted!

        # the sorting procedure is black magic. Don't touch unless you're smarter than me.
        if argsort:  # returns indicies that would sort the array
            # weird try structure because this view is very tempermental!
            try:
                idxs = np.argsort(t.view(','.join(['float64' for _ in xrange(min(t.shape))])),
                                  order=['f%d' % i for i in xrange(min(t.shape))], axis=0)
            except ValueError:  # sort with other side
                idxs = np.argsort(t.view(','.join(['float64' for _ in xrange(max(t.shape))])),
                                  order=['f%d' % i for i in xrange(max(t.shape))], axis=0)

            return idxs[:, 0]

        try:
            t = np.sort(t.view(','.join(['float64' for _ in xrange(min(t.shape))])),
                        order=['f%d' % i for i in xrange(min(t.shape))], axis=0).view(np.float)
        except ValueError:  # sort with other side
            t = np.sort(t.view(','.join(['float64' for _ in xrange(max(t.shape))])),
                        order=['f%d' % i for i in xrange(max(t.shape))], axis=0).view(np.float)

        return t

    ###Emulator Building and Training###################################################################################

    def build_emulator(self, hyperparams):
        """
        Initialization of the emulator from recovered training data. Calls submethods depending on "self.method"
        :param hyperparams
            A dictionary of hyperparameter kwargs for the emulator
        :return: None
        """

        if self.method == 'gp':
            self._build_gp(hyperparams)
        else:  # an sklearn method
            self._build_skl(hyperparams)

    @abstractmethod
    def _build_gp(self, hyperparams):
        pass

    @abstractmethod
    def _build_skl(self, hyperparams):
        pass

    def _get_initial_guess(self, independent_variable):
        """
        Return the initial guess of the metric for the emulator, based on what the iv is. Guesses are learned from
        previous experiments.
        :param independent_variable:
            Which variable to return the guesses for.
        :return: initial_guesses, a dictionary of the guess for each parameter
        """

        # default
        ig = {'amp': 1}
        ig.update({pname: 0.1 for pname in self._ordered_params})

        if self.obs == 'xi':
            if independent_variable is None:
                ig.update({'amp': 0.481, 'logMmin': 0.1349, 'sigma_logM': 0.089,
                           'logM0': 2.0, 'logM1': 0.204, 'alpha': 0.039,
                           'f_c': 0.041, 'r': 0.040, 'z': 1.0})
            else:
                # could have other guesses for this case, but don't have any now
                # leave this structure in case I make more later
                pass
        elif self.obs == 'wp':
            if independent_variable is None:
                ig = {'logMmin': 1.7348042925, 'f_c': 0.327508062386, 'logM0': 15.8416094906,
                      'sigma_logM': 5.36288382789, 'alpha': 3.63498762588, 'r': 0.306139450843,
                      'logM1': 1.66509412286, 'amp': 1.18212664544, 'z': 1.0}
        else:
            pass  # no other guesses saved yet.

        # remove entries for variables that are being held fixed.
        for key in self.fixed_params.iterkeys():
            if key in ig:
                del ig[key]

        return ig

    def _make_kernel(self, metric={}):
        """
        Helper method to build a george kernel for GP's and kernel-based regressions.
        :param metric:
            Hyperparams for kernel determining relative length scales and amplitudes. Default is empty dict.
            In that case, the initial guesses from the object will be used.
        :return:
            A george ExpSquredKernel object with this metric
        """

        if not metric:
            ig = self._get_initial_guess(self.independent_variable)
        else:
            ig = metric  # use the user's initial guesses

        metric = [ig['amp']]
        for pname in self._ordered_params:
            try:
                metric.append(ig[pname])
            except KeyError:
                raise KeyError('Key %s was not in the metric.' % p.name)

        metric = np.array(metric)

        a = metric[0]
        # TODO other kernels?
        return a * ExpSquaredKernel(metric[1:], ndim=self.emulator_ndim)
        #return a * Matern32Kernel(metric[1:], ndim=self.emulator_ndim)


    ###Emulation and methods that Utilize it############################################################################
    def emulate(self, em_params, gp_errs=False):
        """
        Perform predictions with the emulator.
        :param em_params:
            Dictionary of what values to predict at for each param. Values can be
            an array or a float.
        :param gp_errs:
            Boolean, decide whether or not to return the errors from the gp prediction. Default is False.
            Will throw error if method is not gp.
        :return: mu, (errs)
                  The predicted value and the uncertainties for the predictions
                  mu and errs both have shape (npoints,)
        """

        # only has meaning for gp's
        assert not gp_errs or self.method == 'gp'

        input_params = {}
        # input_params.update(self.fixed_params)
        input_params.update(em_params)

        self._check_params(input_params)

        # create the dependent variable matrix
        t_list = [input_params[pname] for pname in self._ordered_params if pname in em_params]
        t_grid = np.meshgrid(*t_list)
        t = np.stack(t_grid).T
        t = t.reshape((-1, self.emulator_ndim))

        # TODO george can sort?
        _t = self._sort_params(t)
        if _t.shape == t.shape:  # protect against weird edge case...
            t = _t

        if len(t.shape) == 1:
            t = np.array([t])

        return self._emulate_helper(t, gp_errs)

    @abstractmethod
    def _emulate_helper(self, t, gp_errs=False):
        pass

    def emulate_wrt_r(self, em_params, r_bin_centers, gp_errs=False):
        """
        Helper function to emulate over r bins.
        :param em_params:
            Parameters to predict at
        :param r_bin_centers:
            Radial bins to predict at
        :param gp_err:
            Boolean, whether or not to use the errors from the GP. Default is False.
            If method is not 'gp', will throw an error
        :return: mu, (errs)
                mu has shape (n_points, len(r_bin_centers))
                errs, if returned, has the same shape
        """
        ep = {}
        ep.update(em_params)
        # extract z from the emulator params, if it's there
        # we will use this to call emulate_wrt_r_z below
        z_bin_centers = np.array([])
        if 'z' in ep:
            z_bin_centers = ep['z']
            if type(z_bin_centers) is float:
                z_bin_centers = np.array([z_bin_centers])
            del ep['z']

        out = self.emulate_wrt_r_z(ep, r_bin_centers, z_bin_centers, gp_errs)

        # Extract depending on if there are errors
        if gp_errs:
            _mu, _errs = out
        else:
            _mu = out

        # now, reshape to have shape (-1, len(r_bin_centers))
        mu = _mu.reshape((-1, r_bin_centers.shape[0]))
        if not gp_errs:
            return mu

        errs = _errs.reshape(mu.shape)
        return mu, errs

    def emulate_wrt_z(self, em_params, z_bin_centers, gp_errs=False):
        """
        Helper function to emulate over z bins.
        :param em_params:
            Parameters to predict at
        :param z_bin_centers:
            Redshift bins to predict at
        :param gp_err:
            Boolean, whether or not to use the errors from the GP. Default is False.
            If method is not 'gp', will throw an error
        :return:mu, (errs)
                mu has shape (n_points, len(z_bin_centers))
                errs, if returned, has the same shape
        """
        ep = {}
        ep.update(em_params)
        r_bin_centers = np.array([])
        if 'r' in ep:
            r_bin_centers = ep['r']
            if type(r_bin_centers) is float:
                r_bin_centers = np.array([r_bin_centers])
            del ep['r']

        out = self.emulate_wrt_r_z(ep, r_bin_centers, z_bin_centers, gp_errs)
        # extract errors if they're returned
        if gp_errs:
            _mu, _errs = out
        else:
            _mu = out

        # now, reshape to have shape (-1, len(z_bin_centers))
        # The swapaxes are necessary to make sure the reshape works properly
        mu = _mu.swapaxes(1, 2).reshape((-1, z_bin_centers.shape[0]))

        if not gp_errs:
            return mu
        errs = _errs.swapaxes(1, 2).reshape(mu.shape)
        return mu, errs

    def emulate_wrt_r_z(self, em_params, r_bin_centers, z_bin_centers, gp_errs=False):
        """
        Conveniance function. Add's 'r' and 'z' to the emulation automatically, as this is the
        most common use case.
        :param em_params:
            Dictionary of what values to predict at for each param. Values can be array
            or float.
        :param r_bin_centers:
            numpy array. Centers of scale bins to predict at, for each point in HOD-space.
            Note this function takes their real-space values, not log-space.
        :param z_bin_centers:
            numpy array. Centers of redshift bins to predict at, for each point in HOD-space
        :param gp_errs:
            Wheter to return the errors of the Gaussian process or not.
        :return: mu, errs (if gp_errs == True)
                both have been reshaped to have shape (-1, len(z_bin_centers), len(r_bin_centers))
        """
        vep = dict(em_params)
        # take the log of r_bin_centers
        rpc = np.log10(r_bin_centers) if np.any(r_bin_centers) else np.array([])  # make sure not to throw an error

        # now, put them into the emulation dictionary.
        for key, val in izip(['r', 'z'], (rpc, z_bin_centers)):
            if key not in self.fixed_params and val.size:  # not fixed and the array is nonzero
                if key not in vep:
                    vep[key] = val
                else:
                    raise ValueError("The parameter %s has been specified twice in emulate_wrt_r_z!" % key)

        # now, emulate.
        out = self.emulate(vep, gp_errs)
        if gp_errs:
            _mu, _errs = out
        else:
            _mu = out

        # account for weird binning isues.
        if not z_bin_centers.shape[0]:
            if not r_bin_centers.shape[0]:
                mu = _mu.reshape((-1, 1, 1))
            else:
                mu = _mu.reshape((-1, 1, r_bin_centers.shape[0]))
        elif not r_bin_centers.shape[0]:
            mu = _mu.reshape((-1, z_bin_centers.shape[0], 1))
        else:
            mu = _mu.reshape((-1, z_bin_centers.shape[0], r_bin_centers.shape[0]))

        if not gp_errs:
            return mu
        errs = _errs.reshape(mu.shape)
        return mu, errs

    # TODO Jeremey keeps konwn uncertainties, I should do the same here, or near to here.
    def estimate_uncertainty(self, truth_dir, N=None):
        """
        Estimate the uncertainty of the emulator by comparing to a "test" box of true values.
        :param truth_dir:
            Name of a directory of true test values, of the same format as the train_dir
        :param N:
            Number of points to compare to. If None (default) will use all points. Else will select random sample.
        :return:
            covariance matrix with dim n_binsxn_bins. Will only have diagonal elemtns of est. uncertainties.
        """
        rms_err = self.goodness_of_fit(truth_dir, N, statistic='rms')

        return np.diag(rms_err ** 2)

    # only predicts wrt r. don't know if that's an issue.
    def goodness_of_fit(self, truth_dir, N=None, statistic='r2'):
        """
        Calculate the goodness of fit of an emulator as compared to some validation data.
        :param truth_dir:
            Directory structured similarly to the training data, but NOT used for training.
        :param N:
            Number of points to use to calculate G.O.F. measures. "None" tests against all values in truth_dir. If N
            is less than the number of points, N are randomly selected. Default is None.
        :param statistic:
            What G.O.F. statistic to calculate. Default is r2. Other options are rmsfd, abs(olute), and rel(ative).
        :return: values, a numpy arrray of the calculated statistics at each of the N training points.
        """
        assert statistic in {'r2', 'rms', 'rmsfd', 'abs', 'log_abs', 'frac', 'log_frac'}
        if N is not None:
            assert N > 0 and int(N) == N

        sub_dirs, _  = self._get_z_subdirs(truth_dir, fixed_zs=self.fixed_params.get('z', None))

        x, y, _, _ = self.get_data(truth_dir, {}, self.fixed_params, self.independent_variable)

        bins, _, _, _ = global_file_reader(sub_dirs[0])
        bin_centers = (bins[1:] + bins[:-1]) / 2
        scale_nbins = len(bin_centers)

        y = y.reshape((-1, scale_nbins))

        np.random.seed(int(time()))

        if N is not None:  # make a random choice
            idxs = np.random.choice(x.shape[0], N, replace=False)

            x, y = x[idxs], y[idxs]
        pred_y = self._emulate_helper(x, False)
        pred_y = pred_y.reshape((-1, scale_nbins))

        # TODO untested
        if np.any(bin_centers != self.scale_bin_centers):
            bin_centers = bin_centers[self.scale_bin_centers[0] <= bin_centers <= self.scale_bin_centers[-1]]
            new_mu = []
            for mean in pred_y:
                xi_interpolator = interp1d(self.scale_bin_centers, mean, kind='slinear')
                interp_mean = xi_interpolator(bin_centers)
                new_mu.append(interp_mean)
            pred_y = np.array(new_mu)
            y = y[:, self.scale_bin_centers[0] <= bin_centers <= self.scale_bin_centers[-1]]

        if statistic == 'rmsfd':
            return np.sqrt(np.mean((((pred_y - y) ** 2) / (y ** 2)), axis=0))

        elif statistic == 'rms':
            return np.sqrt(np.mean(((pred_y - y) ** 2), axis=0))

        # TODO sklearn methods can do this themselves. But i've already tone the prediction!
        elif statistic == 'r2':  # r2
            SSR = np.sum((pred_y - y) ** 2, axis=0)
            SST = np.sum((y - y.mean(axis=0)) ** 2, axis=0)

            return 1 - SSR / SST

        elif statistic == 'abs':
            return 10 ** pred_y - 10 ** y
        elif statistic == 'log_abs':
            return pred_y - y
            # return np.mean((pred_y - y), axis=0)
        elif statistic == 'log_frac':  # 'rel'
            return (pred_y - y) / y
            # return np.mean((pred_y - y) / y, axis=0)
        else:  # 'frac'
            return (10 ** pred_y - 10 ** y) / (10 ** y)

    @abstractmethod
    def train_metric(self, **kwargs):
        pass

    # TODO this feature is not super useful anymore, and also is poorly defined w.r.t non gp methods.
    # did a lot of work on it tho, maybe i'll leave it around...?
    # TODO this feature is no longer correct with EC
    def _loo_errors(self, y, t):
        """
        Calculate the LOO Jackknife error matrix. This is implemented using the analytic LOO procedure,
        which is much faster than re-doing an inversion for each sample. May be useful if the GP's matrix is not
        accurate.
        :param y:
            Values of the independent variable for the training points, used in the prediction.
        :param t:
            Values of the dependant variables to predict at.
        :return:
            jk_cov: a covariance matrix with the dimensions of cov.
        """
        # from time import time

        assert self.method == 'gp'

        if isinstance(self, ExtraCrispy):
            emulator = self._emulators[0]  # hack for EC, do somethign smarter later
        else:
            emulator = self._emulator

        # We need to perform one full inverse to start.
        K_inv_full = emulator.solver.apply_inverse(np.eye(emulator._alpha.size),
                                                   in_place=True)

        # TODO deepcopy?
        x = self.x[:]

        N = K_inv_full.shape[0]

        mus = np.zeros((N, t.shape[0]))
        # t0 = time()

        # iterate over training points to leave out
        for idx in xrange(N):
            # swap the values of the LOO point and the last point.
            x[[N - 1, idx]] = x[[idx, N - 1]]
            y[[N - 1, idx]] = y[[idx, N - 1]]

            K_inv_full[[idx, N - 1], :] = K_inv_full[[N - 1, idx], :]
            K_inv_full[:, [idx, N - 1]] = K_inv_full[:, [N - 1, idx]]

            # the inverse of the LOO GP
            # formula found via MATH
            K_m_idx_inv = K_inv_full[:N - 1, :][:, :N - 1] \
                          - np.outer(K_inv_full[N - 1, :N - 1], K_inv_full[:N - 1, N - 1]) / K_inv_full[N - 1, N - 1]

            alpha_m_idx = np.dot(K_m_idx_inv, y[:N - 1] - emulator.mean(x[:N - 1]))

            Kxxs_t = emulator.kernel.value(t, x[:N - 1])

            # Store the estimate for this LOO GP
            mus[idx, :] = np.dot(Kxxs_t, alpha_m_idx) + emulator.mean(t)

            # restore the original values for the next loop
            x[[N - 1, idx]] = x[[idx, N - 1]]
            y[[N - 1, idx]] = y[[idx, N - 1]]

            K_inv_full[[idx, N - 1], :] = K_inv_full[[N - 1, idx], :]
            K_inv_full[:, [idx, N - 1]] = K_inv_full[:, [N - 1, idx]]

        # return the jackknife cov matrix.
        cov = (N - 1.0) / N * np.cov(mus, rowvar=False)
        if mus.shape[1] == 1:
            return np.array([[cov]])  # returns float in this case
        else:
            return cov


class OriginalRecipe(Emu):
    """Emulator that emulates with only one learner that is trained on all training data. The "naive" approach. """

    def _build_gp(self, hyperparams):
        """
        Initialize the GP emulator.
        :param hyperparams:
            Key word parameters for the emulator
        :return: None
        """
        # TODO could use more of the hyperparams...
        metric = hyperparams['metric'] if 'metric' in hyperparams else {}
        kernel = self._make_kernel(metric)

        self._emulator = george.GP(kernel)
        # gp = george.GP(kernel, solver=george.HODLRSolver, nleaf=x.shape[0]+1,tol=1e-18)

        self._emulator.compute(self.x, self.yerr, sort=False)  # NOTE I'm using a modified version of george!

    def _build_skl(self, hyperparams):
        """
        Build a scikit learn emulator
        :param hyperparams:
            Key word parameters for the emulator
        :return: None
        """
        skl_methods = {'gbdt': GradientBoostingRegressor, 'rf': RandomForestRegressor, \
                       'svr': SVR, 'krr': KernelRidge}

        if self.method in {'svr', 'krr'}:  # kernel based method
            metric = hyperparams['metric'] if 'metric' in hyperparams else {}
            kernel = self._make_kernel(metric)
            if 'metric' in hyperparams:
                del hyperparams['metric']
            # slight difference in use for these saldy
            if self.method == 'svr':
                hyperparams['kernel'] = kernel.value
            else:  # krr
                hyperparams['kernel'] = lambda x1, x2: kernel.value(np.array([x1]), np.array([x2]))

        self._emulator = skl_methods[self.method](**hyperparams)
        self._emulator.fit(self.x, self.y)

    def _emulate_helper(self, t, gp_errs):
        """
        Helper function that takes a dependent variable matrix and makes a prediction.
        :param t:
            Dependent variable matrix. Assumed to be in the order defined by ordered_params
        :param gp_errs:
            Whether or not to return errors in the gp case
        :return:
            mu, err (if gp_errs True). Predicted value for dependetn variable t.
            mu and err both have shape (t.shape[0])
        """
        if self.method == 'gp':
            if gp_errs:
                mu, cov = self._emulator.predict(self.y, t, mean_only=False)
                return mu+self.y_hat, np.diag(cov)
            else:
                return self._emulator.predict(self.y, t, mean_only=True)+self.y_hat
        else:
            return self._emulator.predict(t)+self.y_hat

    def train_metric(self, **kwargs):
        """
        Train the metric parameters of the GP. Has a spotty record of working.
        Best used as used in lowDimTraining.
        If attempted to be used with an emulator that is not GP, will raise an error.
        :param kwargs:
            Kwargs that will be passed into the scipy.optimize.minimize
        :return: success: True if the training was successful.
        """

        # TODO kernel based methods may want to use this...
        assert self.method == 'gp'

        # move these outside? hm.
        def nll(p):
            # Update the kernel parameters and compute the likelihood.
            # params are log(a) and log(m)
            self._emulator.kernel[:] = p
            ll = self._emulator.lnlikelihood(self.y, quiet=True)

            # The scipy optimizer doesn't play well with infinities.
            return -ll if np.isfinite(ll) else 1e25

        # And the gradient of the objective function.
        def grad_nll(p):
            # Update the kernel parameters and compute the likelihood.
            self._emulator.kernel[:] = p
            return -self._emulator.grad_lnlikelihood(self.y, quiet=True)

        p0 = self._emulator.kernel.vector
        results = op.minimize(nll, p0, jac=grad_nll, **kwargs)
        # results = op.minimize(nll, p0, jac=grad_nll, method='TNC', bounds =\
        #   [(np.log(0.01), np.log(10)) for i in xrange(ndim+1)],options={'maxiter':50})
        print results

        self._emulator.kernel[:] = results.x
        self._emulator.recompute()
        # self.metric = np.exp(results.x)

        return results.success


def get_leaves(kdtree):
    """
    Helper function for recursively retriving the leaves of a KDTree
    :param: kdtree
        instance of KDTree to recover leaves of.
    :return: leaves, a list of  numpy arrays of shape (experts, points_per_expert), of leaves.
    """
    points_per_expert = kdtree.leafsize
    leaves_list = []
    get_leaves_helper(kdtree.tree, leaves_list)

    return [np.array(l) for l in leaves_list]


def get_leaves_helper(node, leaves):
    """
    Meta helper function. Recursively
    """
    if isinstance(node, KDTree.leafnode):
        leaves.append(node.idx)
    else:
        get_leaves_helper(node.less, leaves)
        get_leaves_helper(node.greater, leaves)


class ExtraCrispy(Emu):
    """Emulator that emulates with a mixture of expert learners rather than a single one."""

    def __init__(self, training_dir, experts, overlap=1, partition_scheme='random', **kwargs):
        """
        Similar initialization as the superclass with one additional parameter: Em_param
        :param training_dir:
            See above in EMu
        :param experts:
            number of experts to use in the mixture. Must be an integer greater than 1.
        :param overlap:
            overlap in training points between experts. For example, if overlap=2, each datapoint will be in
            2 experts. Default is 1, no overlap.
        :param kwargs:
            As in Emu
        """

        assert experts > 1 and int(experts) == experts  # no point in having less than this
        # TODO experts max value?
        # no point in having overlap the same as experts. You just have experts-many identical gps!
        assert experts > overlap > 0 and int(overlap) == overlap
        assert partition_scheme in {'kdtree', 'random'}

        self.experts = int(experts)
        self.overlap = int(overlap)
        self.partition_scheme = partition_scheme

        super(ExtraCrispy, self).__init__(training_dir, **kwargs)

    def load_training_data(self, training_dir):
        """
        Read the training data for the emulator and attach it to the object.
        :param training_dir:
            Directory where training data from trainginData is stored.
        :param fixed_params:
            Parameters to hold fixed. Only available if data in training_dir is a full hypercube, not a latin hypercube.
        :return: None
        """
        super(ExtraCrispy, self).load_training_data(training_dir)
        self.y+=self.y_hat #need to change this later

        # now, parition the data as specified by the user
        # note that ppe does not include overlap
        points_per_expert = int(1.0 * self.x.shape[0] * self.overlap / self.experts)

        _x = np.zeros((self.experts, points_per_expert, self.x.shape[1]))
        _y = np.zeros((self.experts, points_per_expert))
        _yerr = np.zeros_like(_y)

        if self.partition_scheme == 'random':
            shuffled_idxs = range(self.y.shape[0])
            np.random.shuffle(shuffled_idxs)

            # select potentially self.overlapping subets of the data for each expert
            for i in xrange(self.experts):
                _x[i, :, :] = np.roll(self.x[shuffled_idxs, :], i * points_per_expert / self.overlap, 0)[
                              :points_per_expert, :]
                _y[i, :] = np.roll(self.y[shuffled_idxs], i * points_per_expert / self.overlap, 0)[:points_per_expert]
                _yerr[i, :] = np.roll(self.yerr[shuffled_idxs], i * points_per_expert / self.overlap, 0)[
                              :points_per_expert]

        else:  # KDTree
            # whiten so all distances are the same
            normed_x = (self.x - self.x.min(axis=0)) / self.x.max(axis=0)
            normed_x[np.isnan(normed_x)] = 0.0
            kdtree = KDTree(normed_x, leafsize=points_per_expert / self.overlap)
            leaves = get_leaves(kdtree)

            prev_idx, curr_idx = 0, 0
            # If points cannot be evenly divided, there'll be some skipped ones.
            # We'll add them in at the end.
            n_missed = np.sum([(self.overlap * len(leaf) % self.experts) / self.overlap for leaf in leaves])

            missed_points = np.zeros(n_missed, dtype=int)
            missed_idx = 0

            # leaves can have different sizes, so we have to treat each leaf differently
            for i, leaf in enumerate(leaves):
                shuffled_idxs = range(leaf.shape[0])
                np.random.shuffle(shuffled_idxs)

                leaf_ppe = int(1.0 * self.overlap * leaf.shape[0] / self.experts)
                curr_idx = prev_idx + leaf_ppe

                # select potentially overlapping subets of the data for each expert
                for j in xrange(self.experts):
                    _x[j, prev_idx:curr_idx, :] = \
                        np.roll(self.x[leaf[shuffled_idxs], :], j * leaf_ppe / self.overlap, 0)[:leaf_ppe, :]
                    _y[j, prev_idx:curr_idx] = \
                        np.roll(self.y[leaf[shuffled_idxs]], j * leaf_ppe / self.overlap, 0)[:leaf_ppe]
                    _yerr[j, prev_idx:curr_idx] \
                        = np.roll(self.yerr[leaf[shuffled_idxs]], j * leaf_ppe / self.overlap, 0)[:leaf_ppe]

                prev_idx = curr_idx
                nm = (self.overlap * leaf.shape[0] % self.experts) / self.overlap
                if nm != 0:
                    missed_points[missed_idx:missed_idx + nm] = leaf[shuffled_idxs][-nm:]
                    missed_idx += nm

            # now, distribute leftover points over experts
            if n_missed > 0:
                missed_ppe = int(1.0 * n_missed * self.overlap / self.experts)

                curr_idx = prev_idx + missed_ppe

                for i in xrange(self.experts):
                    _x[i, prev_idx:curr_idx, :] = \
                        np.roll(self.x[missed_points, :], i * missed_ppe / self.overlap, 0)[:missed_ppe, :]
                    _y[j, prev_idx:curr_idx] = \
                        np.roll(self.y[missed_points], i * missed_ppe / self.overlap, 0)[:missed_ppe]
                    _yerr[j, prev_idx:curr_idx] \
                        = np.roll(self.yerr[missed_points], i * missed_ppe / self.overlap, 0)[:missed_ppe]

                # now, to cover the meta-missed ones, just fill in points until they're full
                while curr_idx != self.x.shape[1]:
                    prev_idx = curr_idx
                    curr_idx += 1
                    for j in xrange(self.experts):
                        _x[j, prev_idx:curr_idx, :] = \
                            np.roll(self.x[missed_points, :], (j + i) * missed_ppe / self.overlap, 0)[:1, :]
                        _y[j, prev_idx:curr_idx] = \
                            np.roll(self.y[missed_points], (j + i) * missed_ppe / self.overlap, 0)[:1]
                        _yerr[j, prev_idx:curr_idx] \
                            = np.roll(self.yerr[missed_points], (j + i) * missed_ppe / self.overlap, 0)[:1]

        # now attach these final versions
        self.x = _x
        self.y = _y
        self.yerr = _yerr
        self.y_hat = self.y.mean(axis = 1)
        self.y-=self.y_hat.reshape((-1, 1))

    def _build_gp(self, hyperparams):
        """
        Initialize the GP emulator using an MOE model.
        :param hyperparams:
            Key word parameters for the emulator
        :return: None
        """
        if 'metric' in hyperparams:
            metric = hyperparams['metric']
            del hyperparams['metric']
        else:
            metric = {}
        kernel = self._make_kernel(metric)

        # now, make a list of emulators
        self._emulators = []

        for _x, _yerr in izip(self.x, self.yerr):
            emulator = george.GP(kernel)

            emulator.compute(_x, _yerr, sort=False, **hyperparams)  # NOTE I'm using a modified version of george!
            self._emulators.append(emulator)

    def _build_skl(self, hyperparams):
        """
        Build a scikit learn emulator using a mixtrue of experts.
        :param hyperparams:
            Key word parameters for the emulator
        :return: None
        """
        skl_methods = {'gbdt': GradientBoostingRegressor, 'rf': RandomForestRegressor, \
                       'svr': SVR, 'krr': KernelRidge}

        # Same kernel concerns as above.
        if self.method in {'svr', 'krr'}:  # kernel based method
            metric = hyperparams['metric'] if 'metric' in hyperparams else {}
            kernel = self._make_kernel(metric)
            if 'metric' in hyperparams:
                del hyperparams['metric']
            if self.method == 'svr':  # slight difference in these, sadly
                hyperparams['kernel'] = kernel.value
            else:  # krr
                hyperparams['kernel'] = lambda x1, x2: kernel.value(np.array([x1]), np.array([x2]))

        self._emulators = [skl_methods[self.method](**hyperparams) for i in xrange(self.experts)]

        for i, (emulator, _x, _y) in enumerate(izip(self._emulators, self.x, self.y)):
            emulator.fit(_x, _y)

    def _emulate_helper(self, t, gp_errs=False):
        """
        Helper function that takes a dependent variable matrix and makes a prediction.
        :param t:
            Dependent variable matrix. Assumed to be in the order defined by ordered_params
        :param gp_errs:
            Whether or not to return errors in the gp case
        :return:
            mu, err (if gp_errs True). Predicted value for dependetn variable t.
            mu and err both have shape (npoints*self.redshift_bin_centers*self.scale_bin_centers)
        """
        mu = np.zeros((self.experts, t.shape[0]))  # experts down, t deep
        err = np.zeros_like(mu)

        for i, (emulator, _y,_yhat) in enumerate(izip(self._emulators, self.y, self.y_hat)):
            if self.method == 'gp':
                local_mu, local_cov = emulator.predict(_y, t, mean_only=False)
                local_err = np.sqrt(np.diag(local_cov))
            else:
                local_mu = emulator.predict(t)
                local_err = 1.0  # weight with this instead of the errors.

            mu[i, :] = local_mu+_yhat
            err[i, :] = local_err

        # now, combine with weighted average
        combined_var = np.reciprocal(np.sum(np.reciprocal(err ** 2), axis=0))
        combined_mu = combined_var * np.sum(np.reciprocal(err ** 2) * mu, axis=0)

        # Reshape to be consistent with my other implementation
        if not gp_errs:
            return combined_mu
        return combined_mu, np.sqrt(combined_var)

    # TODO could make this learn the metric for other kernel based emulators...
    def train_metric(self, **kwargs):
        """
        Train the emulator. Has a spotty record of working. Better luck may be had with the NAMEME code.
        :param kwargs:
            Kwargs that will be passed into the scipy.optimize.minimize
        :return: success: True if the training was successful.
        """

        assert self.method == 'gp'

        # emulators is a list containing refernces to the same object. this should still work!

        # move these outside? hm.
        def nll(p):
            # Update the kernel parameters and compute the likelihood.
            # params are log(a) and log(m)
            ll = 0
            for emulator, _y in izip(self._emulators, self.y):
                emulator.kernel[:] = p
                ll += emulator.lnlikelihood(_y, quiet=True)

            # The scipy optimizer doesn't play well with infinities.
            return -ll if np.isfinite(ll) else 1e25

        # And the gradient of the objective function.
        def grad_nll(p):
            # Update the kernel parameters and compute the likelihood.
            gll = 0
            for emulator, _y in izip(self._emulators, self.y):
                emulator.kernel[:] = p
                gll += emulator.grad_lnlikelihood(_y, quiet=True)
            return -gll

        p0 = self._emulators[0].kernel.vector
        results = op.minimize(nll, p0, jac=grad_nll, **kwargs)
        # results = op.minimize(nll, p0, jac=grad_nll, method='TNC', bounds =\
        #   [(np.log(0.01), np.log(10)) for i in xrange(ndim+1)],options={'maxiter':50})

        for emulator in self._emulators:
            emulator.kernel[:] = results.x
            emulator.recompute()

        return results.success
